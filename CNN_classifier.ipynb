{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2e1f3",
   "metadata": {},
   "source": [
    "## First Go at Classification Using a Convolutional Neural Network\n",
    "\n",
    "In our EDA, we saw that there is a variety of sizes, color modes, and class sizes within our dataset. This means we will likely need to do some preprocessing on our data before creating a classification algorithm. Luckily, TensorFlow has many in-built functions which take care of most of these steps for us. Before adding augmentation, it is good to perform a baseline test of how well a classification model performs on a dataset. Let's now go ahead and create a simple convolutional neural network using TensorFlow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469d73d",
   "metadata": {},
   "source": [
    "First of all, let's load up the data using a Keras utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_height = 800\n",
    "img_width = 800\n",
    "data_dir = 'rgb-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba59474",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=1337,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=1337,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02449522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf51ac4",
   "metadata": {},
   "source": [
    "Now it's time to decide on a model architecture! \n",
    "\n",
    "My first thought was to emulate the famous LeNet-5 architecture, which consists of a series of convolutional and pooling layers followed by fully connected layers. Firstly, we there are three convolutional layers using 5x5 kernels with 6, 16, and 120 channels. The activation function used is the hyperbolic tangent (tanh). Between the first two and second two layers are average pooling layers with sigmoid activations. After these layers, the output is flattened, and fed through two fully-connected layers of sizes 84 and 10 with tanh and SoftMax activations respectively.\n",
    "\n",
    "LeNet-5 was groundbreaking in its time and laid the foundation for modern CNNs. However, to adapt to the complexities of contemporary computer vision tasks and leverage the advancements in deep learning, I decided to modernize the architecture. The updated design incorporates smaller 3x3 convolutional kernels, which are computationally less expensive than the larger 5x5 filters used by LeNet. Furthermore, the 3x3 kernel already incorporates all the directional information that a larger filter can. Such a kernel will also feature fewer trainable parameters, and may help reduce overfitting.\n",
    "\n",
    "Additionally, it replaces average pooling with max pooling, which effectively retains only the dominant features. This makes sense for architectural classification, as each style has a few dominant characteristics which already allow us to recognize it (eg classical style can be recognized through columsn, modernism through bare walls).\n",
    "\n",
    "Finally, I replaced all the activation functions apart from SoftMax with ReLU. At first, this was purely due to computational limitations as I was running the training on my laptop. However, I realized that ReLUs also seem to make more sense if we think about the biological inspiration for neural networks. We can imagine a neuron inside a human brain that would only \"fire\" if an ionic signal with enough voltage comes its way, much like an on-off switch. In this sense, we have some biological plausibility for using ReLUs in artificial neural networks, whereas it is difficult to think of any natural mechanism to reproduce the behavior of sigmoid or hyperbolic tangent functions.\n",
    "\n",
    "With these changes in mind, I am ready to define the model architecture with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    \n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(120, 3, padding='same', activation='relu'),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "    \n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90668398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d4843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training and validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(epochs_range, acc, label='Training Accuracy', marker='o', linestyle='-')\n",
    "axs[0].plot(epochs_range, val_acc, label='Validation Accuracy', marker='o', linestyle='-')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_title('Training and Validation Accuracy')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='-')\n",
    "axs[1].plot(epochs_range, val_loss, label='Validation Loss', marker='o', linestyle='-')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_title('Training and Validation Loss')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10163087",
   "metadata": {},
   "source": [
    "We can see that the training loss consistenly decreases while training accuracy increases. This is exactly what we hope to achieve in machine learning! However, we can also see that the story is very different for our validation data. The validation accuracy stays flat while the loss keeps increasing. So what is going on? Well, this is a problem known as overfitting. This indicates that while the model can learn to classify images in the training dataset very well, it is unable to generalize to images it has not seen before. This indicates that we should explore techniques to avoid this, and luckily TensorFlow has many functions that can do this for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490221c6",
   "metadata": {},
   "source": [
    "Let's do some augmentation. We can define this using Keras preprocessing layers, which we can then include in our model. The goal is to use augmentations which would still give us believable and relevant images, so we won't use vertical flips in this case, as we would not expect to ever see an upside down building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",input_shape=(img_height, img_width, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "improved_model = Sequential([\n",
    "  augmentations,\n",
    "    \n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    \n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "    \n",
    "  layers.Conv2D(120, 3, padding='same', activation='relu'),\n",
    "    \n",
    "  layers.Dropout(0.2),\n",
    "  \n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "    \n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f334e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = improved_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72615df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the training and validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(epochs_range, acc, label='Training Accuracy', marker='o', linestyle='-')\n",
    "axs[0].plot(epochs_range, val_acc, label='Validation Accuracy', marker='o', linestyle='-')\n",
    "axs[0].legend(loc='upper left')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_title('Training and Validation Accuracy')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='-')\n",
    "axs[1].plot(epochs_range, val_loss, label='Validation Loss', marker='o', linestyle='-')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_title('Training and Validation Loss')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
